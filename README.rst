
Rational models of categorization
=================================

Here are implemented several versions of the so-called rational model of
categorization. Executing any of the python files here will launch a test/demo
of one of these classic models.

All these models assume that stimuli are generated by a mixture of underlying
distributions of unknown size. These distributions are Gaussian on continuous
stimulus dimensions, and multinomial on discrete stimulus dimensions. The prior
over the number of mixture components is given via a Dirichlet distribution.

They bear a strong resemblance to naive Bayes.

Requirements
------------
These should all run in `Python`_ >= 2.5, with `numpy`_ and `scipy`_. The
plotting in some of the demos will require `matplotlib`_.

.. _Python: http://www.python.org
.. _numpy: http://www.numpy.org
.. _scipy: http://www.scipy.org
.. _matplotlib: http://matplotlib.sourceforge.net


The original: rational.py
-------------------------
Anderson's original model is available here. In Figure 1 of Anderson (1991)
[Anderson]_ walks through the model's inner workings on each time step as it
learns the classic Medin & Schaffer (1978) [MedSchaff]_ task. Running the file
in python launches a demo which performs the same task, finding the same
answers at each step.

Becuase this model was developed before advanced techniques for approximating
intractable Bayesian posteriors were in wide use, the model views stimuli
sequentially and assigns them deterministically to the cluster that was most
likely to have generated them.

.. [Anderson] Anderon, J. R. (1991). "The adaptive nature of human
   categorization." *Psychological Review*, **98**:409-429.

.. [MedSchaff] Medin, D. L. and Schaffer, M. M. (1978). "Context
   Theory of Classification Learning." *Psychological Review*, **85**:207-238.


The "more rational" model.
--------------------------
Recently, Sanborn, Griffiths and Navarro (2006) [sgn]_ brought the model up to
date with two methods for approximating the full posterior over possible
partitions of the stimuli. I have implemented the basic model in the document
``particle.py``, and implemented each of these approximation methods as
extensions. Since the Anderson model is a special use of the "more rational"
model, it can also be run here. Psychologists are particularly interested in
the particle filter because it operates on-line, which is generally taken as a
necessity for a Psychologically plausible algorithm.

Gibbs Sampling 
    All items are assigned arbitrarily. Sampling proceeds by removing each item
    one by one and relabeling it probabilistically. In the limit, the
    likelihood of a given partition of the stimuli is given by the number of
    times it is visited. I have implemented this in ``GibbsSampler.py``.
    Running the script launches a demo, which runs the sampler for a few
    hundred iterations on the Zeithamova and Maddox (2009) [zm]_ dataset,
    printing out its partition at each stage.
    
Particle filtering.
    Items are viewed sequentially, as in the Anderson (1991) model, but the
    model tracks many hypotheses about the correct partition, and at each stage
    resamples from its own existing samples. I have implemented this in
    ``filter.py``. Running this script launches a demo which runs the [zm]_
    task with 6 particles, plotting each particle's partition at the end.

.. [sgn] Sanborn, A. N., Griffiths T. L., and Navarro, D. J. (2006).
   "A More Rational Model of Categorization." *Proceedings of the 28th Annual
   Conference of the Cognitive Science Society*.

.. [zm] Zeithamova, D. and Maddox, W. T. (2009). "Learning mode and exemplar
   sequencing in unsupervised category learning." *Journal of Experimental
   Psychology: Learning, Memory, and Cognition*, **35**:731-757.

